Written Questions

Q1. Run the web crawler using the configurations located at src/main/config/written_question_1a.json and
    src/main/config/written_question_1b.json. The only difference between these configurations is that one always uses
    the sequential crawler and the other always uses the parallel crawler. Inspect the profile output in
    profileData.txt.

    If you are using a multi-processor computer, you should notice that SequentialWebCrawler#crawl and
    ParallelWebCrawler#crawl took about the same amount of time, but PageParserImpl#parse took much longer when run with
    the ParallelWebCrawler.

    Why did the parser take more time when run with ParallelWebCrawler?

 Answer 1. Reason being the profiler adds the time taken by parse method in all threads.

Q2. Your manager ran your crawler on her old personal computer, using the configurations from Q1, and she notices that
    the sequential crawler actually outperforms the parallel crawler. She would like to know why.

    (a) Suggest one reason why the sequential web crawler was able to read more web pages than the parallel crawler.
        (Hint: Try setting "parallelism" to 1 in the JSON configs to simulate your manager's computer.)

    Answer 2a.A parallel web crawler uses more resources on a single-threaded computer because the primary condition for optimal functioning is a more significant number of threads. That is why it does not perform well on an old computer. On the contrary, a sequential web crawler was created to work sequentially without using more than one thread simultaneously; that's why it works better on a computer with only one thread.

    (b) Suggest one scenario in which the parallel web crawler will almost certainly perform better than the sequential
        crawler. Why will it perform better?

    Answer 2b.The parallel web crawler will outperform the sequential web crawler on a multi-core computer. A multi-core computer does have a more significant number of threads for computing, so it benefits from the Parallel crawl functionality.

Q3. Analyze your method profiler through the lens of Aspect Oriented Programming, by answering the following questions:

    (a) What cross-cutting concern is being addressed by the com.udacity.webcrawler.profiler.Profiler class?

    Answer 3a. The performance measure of the Profiler class is a property of a cross-cutting concern.

    (b) What are the join points of the Profiler in the web crawler program?

    Answer 3b. Methods annotated with @Profiled are Join points.

Q4. Identify three (3) different design patterns used in this project, and explain which interfaces, classes, and/or
    libraries use or implement those design patterns.

    For each pattern, name one thing about the pattern that you LIKED, and one thing you DISLIKED. If you did not like
    anything, you can name two things you disliked.

    Answer 4a. Dependency Injection: It's used in the WebCrawlerMain and Profiler classes. This design pattern simplifies tests and makes classes more modular, but it increases the project's number of classes and/or interfaces.

               Builder Pattern: It's used in the CrawlerConfiguration, CrawlResult, ParserModule classes, and the PageParser interface.
               It simplifies the creation of instances of complex constructor classes, but it significantly increases the amount of code.

               Proxy Pattern - It's used in ProfilerImpl class. It shows a suitable method for operating with interfaces in runtime but makes the code difficult to comprehend.